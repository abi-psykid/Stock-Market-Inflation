{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "954696dc",
   "metadata": {},
   "source": [
    "# NEWS SENTIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9445889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import warnings\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from pyspark.sql.functions import col, lit, regexp_replace, lower\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, unix_timestamp, to_date\n",
    "from pyspark.sql import Window\n",
    "\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35ba2823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a spark session\n",
    "\n",
    "spark = SparkSession.builder.appName('DM').config(\"spark.executor.heartbeatInterval\", \"110000ms \").getOrCreate() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60271d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating path variables for all available data\n",
    "\n",
    "data_path = \"\" # Path to Data folder\n",
    "lexicon_path = \"\" # Path to lexicon data for sentiment analysis\n",
    "news_path = \"\" # Path to news data\n",
    "main_data_path = data_path + \"\\\\final_data.csv\" # Path to DJUSCG index data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ba8895",
   "metadata": {},
   "source": [
    "SENTIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01ff7130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The news dataset and main dataset are converted into Spark Data Frame\n",
    "\n",
    "news_dataFrame = spark.read.csv(news_path, header=True, inferSchema=True)\n",
    "df_main = spark.read.csv(main_data_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d262cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the unncessary columns in news dataset\n",
    "\n",
    "dropCols = ('author','title','url','section','publication')\n",
    "news_dataFrame = news_dataFrame.drop(*dropCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce4c4d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the null values from the news dataset\n",
    "\n",
    "news_dataFrame = news_dataFrame.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ba060d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to extract the date from a given String\n",
    "\n",
    "def extractDate(sentence):\n",
    "    x = re.search(\"\\d{4}-\\d{1,2}-\\d{1,2}\", sentence)\n",
    "    if x:\n",
    "        dateString =  x.group()\n",
    "        return dateString\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "udf_newDate = F.udf(lambda z: extractDate(z), StringType())\n",
    "news_dataFrame = news_dataFrame.withColumn('newDate', udf_newDate('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aa2c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The extracted date is converted into DateType() format\n",
    "\n",
    "news_dataFrame = news_dataFrame.withColumn('dateFormat', to_date(col('newDate'), 'yyyy-MM-dd')) \\\n",
    "                        .dropna() \\\n",
    "                        .drop('date', 'newDate') \\\n",
    "                        .withColumnRenamed('dateFormat', 'date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b9da37",
   "metadata": {},
   "source": [
    "Lexicon Files Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "521723ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "stock_lex_path = lexicon_path + \"/stock_lex.csv\" \n",
    "stock_lex = pd.read_csv(stock_lex_path)\n",
    "\n",
    "stock_lex['sentiment'] = (stock_lex['Aff_Score'] + stock_lex['Neg_Score'])/2\n",
    "stock_lex = dict(zip(stock_lex.Item, stock_lex.sentiment))\n",
    "stock_lex = {k:v for k,v in stock_lex.items() if len(k.split(' '))==1}\n",
    "stock_lex_scaled = {}\n",
    "for k, v in stock_lex.items():\n",
    "    if v > 0:\n",
    "        stock_lex_scaled[k] = v / max(stock_lex.values()) * 4\n",
    "    else:\n",
    "        stock_lex_scaled[k] = v / min(stock_lex.values()) * -4\n",
    "        \n",
    "positive = []\n",
    "with open(lexicon_path + '/lm_positive.csv', 'r') as f:\n",
    "     reader = csv.reader(f)\n",
    "     for row in reader:\n",
    "         positive.append(row[0].strip())\n",
    "negative = []\n",
    "with open(lexicon_path + '/lm_negative.csv', 'r') as f:\n",
    "     reader = csv.reader(f)\n",
    "     for row in reader:\n",
    "         entry = row[0].strip().split(\" \")\n",
    "         if len(entry) > 1:\n",
    "             negative.extend(entry)\n",
    "         else:\n",
    "             negative.append(entry[0])\n",
    "final_lex = {}\n",
    "final_lex.update({word:2.0 for word in positive})\n",
    "final_lex.update({word:-2.0 for word in negative})\n",
    "final_lex.update(stock_lex_scaled)\n",
    "final_lex.update(sia.lexicon)\n",
    "sia.lexicon = final_lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "366e3f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the dat set contains 2.7 million news articles related to different topics, \n",
    "# we searched for news related to \"Dow J\" keyword\n",
    "\n",
    "\n",
    "news_dataFrame = news_dataFrame.filter(col(\"article\").contains(\"Dow J\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4abafc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that returns the positive, negative, and neutral sentiment scores for a given article / sentence.\n",
    "\n",
    "def sentiment_scores(sentence):\n",
    "    sentiment_dict = sia.polarity_scores(sentence)\n",
    "     \n",
    "    print(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
    "    print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
    "    print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
    "    print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
    " \n",
    "    print(\"Sentence Overall Rated As\", end = \" \")\n",
    " \n",
    "    # sentence.withColumn(\"Sentiment_Score\",(sentiment_dict['compound']))\n",
    "    # decide sentiment as positive, negative and neutral\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        print(\"Positive\")\n",
    " \n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        print(\"Negative\")\n",
    " \n",
    "    else :\n",
    "        print(\"Neutral\")\n",
    "        \n",
    "    return sentiment_dict['compound']\n",
    "\n",
    "udf_sentiment_scores = F.udf(lambda z: sentiment_scores(z), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2051357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Expressions are removed from \"article\" column because they do not possess any sentiment.\n",
    "\n",
    "news_dataFrame = news_dataFrame.select(\"date\", regexp_replace(\"article\", \"[^0-9a-zA-Z_\\- &]+\", \"\").alias('replaced_str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a531c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All letters were converted to lower case.\n",
    "\n",
    "news_dataFrame = news_dataFrame.select('date', lower(col('replaced_str')).alias('replaced_lower_str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a526c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function removes all the stopwords (e.g. and, the, is, an, a, etc.)\n",
    "\n",
    "def myStopwordRemover(df:pyspark.sql.DataFrame, inputColName:str, outputColName:str) -> pyspark.sql.DataFrame:\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    udf_remove_stop_words = F.udf(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    stop = set(stopwords.words('english'))\n",
    "    newDf = df.withColumn(outputColName, udf_remove_stop_words(inputColName))\n",
    "    return newDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "080b5718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The stop words are removed from the dataframe.\n",
    "\n",
    "news_dataFrame = myStopwordRemover(news_dataFrame, 'replaced_lower_str', 'Cleaned_Article')\n",
    "news_dataFrame = news_dataFrame.drop('replaced_lower_str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf95c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sentiment scores have been calculated for each article and are added into their corresponding records.\n",
    "\n",
    "news_dataFrame = news_dataFrame.withColumn(\"Sentiment_Score\", udf_sentiment_scores('Cleaned_Article')) \\\n",
    "                    .drop('Cleaned_Article') \\\n",
    "                    .groupBy('date').avg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "873479e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A pyspark dataframe containing the only column of dates from 02-01-2015 to 30-03-2020\n",
    "\n",
    "import datetime\n",
    "def get_date_df():\n",
    "    initial_date = datetime.date(2015, 1, 2 )\n",
    "    days = 1916\n",
    "    one_day = datetime.timedelta(days=1)\n",
    "    all_days = [{\"date\": initial_date + i * one_day} for i in range(days)]\n",
    "    return spark.createDataFrame(Row(**x) for x in all_days)\n",
    "date_df = get_date_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f8468f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above date dataframe is left joined with news dataframe, so as to have all the missing dates with null values in the news_dataFrame\n",
    "\n",
    "news_dataFrame = date_df.join(news_dataFrame, 'date', 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d75f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The null values added into the news dataframe after join operation are swapped with their previous available day's sentiment score,\n",
    "# if previous sentiment score is not available, then it's sentiment score is assigned to 0\n",
    "# This is done because, it is assusmed as sentiment score doesn't change until the release of next news.\n",
    "\n",
    "Windowspec=Window.orderBy(\"date\")\n",
    "news_dataFrame = news_dataFrame.withColumn('Sentiment Score',F.when(F.isnull(F.col('avg(Sentiment_Score)')),F.lag(F.col('avg(Sentiment_Score)'), 1, 0).over(Windowspec)).otherwise(F.col('avg(Sentiment_Score)')))\n",
    "news_dataFrame = news_dataFrame.withColumn('Final Sentiment Score',F.when(F.isnull(F.col('Sentiment Score')), 0).otherwise(F.col('Sentiment Score')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e816910b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- avg(Sentiment_Score): double (nullable = true)\n",
      " |-- Sentiment Score: double (nullable = true)\n",
      " |-- Final Sentiment Score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_dataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76d9baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns that have been created in the process, but are not useful anymore.\n",
    "\n",
    "news_dataFrame = news_dataFrame.drop('avg(Sentiment_Score)')\n",
    "news_dataFrame = news_dataFrame.drop('Sentiment Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a5f9982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the Date Column in main dataset in pyspark from StringType() to DateTpe()\n",
    "\n",
    "df_main = df_main.withColumn('dateFormat', to_date(unix_timestamp(col('date'), 'dd-MM-yyyy').cast(\"timestamp\"))) \\\n",
    "                .drop('Date') \\\n",
    "                .withColumnRenamed('dateFormat', 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93d7480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The news dataframe is inner joined with main dataframe on \"Date\" column basis,\n",
    "# Inner join joins the columns from both dataframes when the 'Date' column values are same in both the dataframes.\n",
    "\n",
    "df_main = df_main.join(news_dataFrame, ['date'], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be065a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sentiment score values in the column have been changed to maintain their values till 4 decimal places.\n",
    "\n",
    "df_main = df_main.withColumn('Sentiment Score', F.round(F.col('Final Sentiment Score'), 4)) \\\n",
    "                    .withColumnRenamed('date', 'Date') \\\n",
    "                    .drop(F.col('Final Sentiment Score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d91ca0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.coalesce(1).write.mode('overwrite').option('header', True).csv(\"FINAL_DATA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
